---
title: "Replication of Study 'What makes words special? Words as unmotivated cues' by Edmiston & Lupyan (2015, Cognition)"
author: "Replication Author[s]: Sihan Yang (siy009@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

[Link to the project github repository](https://github.com/ucsd-psych201a/edmiston2015)

## Introduction

"Verbal labels, such as the words 'dog' and 'guitar', activate conceptual knowledge more effectively than corresponding environmental sounds, such as a dog bark or a guitar strum, even though both are unambiguous cues to the categories of dogs and guitars (Lupyan & Thompson-Schill, 2012). We hypothesize that this advantage of labels emerges because word-forms, unlike other cues, do not vary in a motivated way with their referent. The sound of a guitar cannot help but inform a listener to the type of guitar making it (electric, acoustic, etc.). The word 'guitar' on the other hand, can leave the type of guitar unspecified. We argue that as a result, labels gain the ability to cue a more abstract mental representation, promoting efficient processing of category members. In contrast, environmental sounds activate representations that are more tightly linked to the specific cause of the sound. Our results show that upon hearing environmental sounds such as a dog bark or guitar strum, people cannot help but activate a particular instance of a category, in a particular state, at a particular time, as measured by patterns of response times on cue-picture matching tasks (Exps. 1–2) and eye-movements in a task where the cues are task-irrelevant (Exp. 3). In comparison, labels activate concepts in a more abstract, decontextualized way—a difference that we argue can be explained by labels acting as 'unmotivated cues'." (the abstract of the original paper Edmiston & Lupyan, 2015)  

In this study, we will replicate Experiment 1A from the original paper. Specifically, we will test whether recognizing an image following a sound cue is slower than recognizing an image following a label cue, and whether this difference is because a sound cues a more specific category than a label. We will use the same experimental paradigm and materials as the original study, but with a larger but with a different sample size and population and an online format instead of the original in-lab setting.

## Methods

### Power Analysis

[TODO] Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size.

### Planned Sample

We recruit 43 participants as in the original study. Participants are recruited on Prolific, prescreened for English fluency and compensated monetarily.

### Materials

"The auditory cues comprised basic-level category labels and environmental sounds for six categories: bird, dog, drum, guitar, motorcycle, and phone. For each category, we obtained two distinct environmental sound cues, e.g., <classical guitar strum>, <electric guitar strum>, and two separate images for each subordinate category, e.g., two electric guitars for <electric guitar strum>, two acoustic guitars for <electric guitar strum>. To control for cue variability, we also used two versions of each spoken category label: one pronounced by a female speaker, one by a male speaker. All auditory cues were equated in duration (600 ms.) and normalized in volume. The images were color photographs (four images per category). The materials, obtained from online repositories, are available for download at http://sapir.psych.wisc.edu/stimuli/ MotivatedCuesExp1A-1B.zip." (Edmiston & Lupyan, 2015)

### Procedure	

Participants are instructed to use their personal computer to complete task in a quite environment absent of distraction. During the experiment, they will be required to make their response by clicking 'y' on the keyboard of their computer for 'Yes', and 'n' for 'No'.

During the experiment, "on each trial participants heard a cue and saw a picture...participants decide as quickly and accurately as possible if the picture they saw came from the same basic-level category as the word or sound they heard...Trials began with a 250 ms. fixation cross followed immediately by the auditory cue, delivered via headphones. The target image appeared centrally 1 s after the offset of the auditory cue and remained visible until a response was made. Each participant completed 6 practice and 384 test trials. If the picture matched the auditory cue (50% of trials) participants were instructed to respond ‘Yes’...(e.g., <cellphone ring> or 'phone' followed by a picture of any phone). Otherwise, they were to press 'No' (e.g., <cellphone ring> or 'phone' followed by a dog). All factors (cue type, congruence) varied randomly within subjects. Auditory feedback (buzz or bleep) was given after each trial." (Edmiston & Lupyan, 2015)

### Analysis Plan

#### Data cleaning and Data Exclusion Rules

Before testing the main hypothesis of the original study, we will first examine the average performance (accuracy) to determine if it is comparable to the original study. This will involve calculating each participant's accuracy and using a t-test to examine whether performance differs qualitatively between the original and new participants.

Trials with response times (RTs) that are too short or too long will be filtered out. The original study applied thresholds of 250 ms and 1500 ms for the lower and upper bounds, respectively. Before applying filtering, we will check if the new data shows a similar RT distribution.

Only RTs for correct responses on matching trials will be used for further analysis, following the original study’s approach.

#### Covariates

The main covariate in this experiment will be "the presence of incongruent sound trials (e.g., hearing a <cell phone ring> and responding ‘Yes’ if a rotary phone was presented) may have led participants to be more careful on these trials, inflating the RTs" (Edmiston & Lupyan, 2015). However, as this factor is not the focus of the experiment to replicate and was addressed in another following experiment in the original study, we will not include it in our analysis.

#### Key Analysis

We will "fit RTs for correct responses on matching trials (‘Yes’ responses)" under three conditions of different cues (verbal label cue, congruent sound cue, incongruent sound cue), "with linear mixed regression using maximum likelihood estimation (Bates, Maechler, Bolker, & Walker, 2013), including random intercepts and random slopes for within-subject factors and random intercepts for repeated items (unique trial types) following the recommendations of Barr, Levy, Scheepers, and Tily (2013)." (Edmiston & Lupyan, 2015). 

We will report the "parameter estimates (b) and confidence intervals for each contrast of interest" (Edmiston & Lupyan, 2015). Significance tests will be calculated "using chi-square tests that compared nested models—models with and without the factor of interest—on improvement in log-likelihood" (Edmiston & Lupyan, 2015).

According to the original study, we predict that the analysis above should show that (1) verbal labels elicit faster responses than sound labels, and (2) congruent sounds elicit faster response than incongruent sounds, with statistical significance (indicated by chi-square test p value less than 0.05).

### Differences from Original Study

#### Sample

While the original study recruited participants on campus, our recruitment is conducted on a public global outsourcing platform, expanding the sampling pool from U.S. college undergraduates to a more diverse population.

#### Procedure

While the original study was conducted in person in a lab, our replication will be hosted online, allowing participants to complete the task independently at a location of their choice, without the supervision of experimenters. In addition, responses will be made using a keyboard rather than a game controller.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.
  
### Comments (from Oct 31st class)
How many factors were manipulated?
- one, which is whether particioants hear a label, an incongurent sound or a congruent sound cue.
How many measures were taken?
- one, reaction time.
Did it use a within-participants or between-participants design?
- within participants.
Were measures repeated?
- yes.
What would have been the consequence of changing one of these decisions? (e.g. moving from within to between participants). Could it have been either way?
- more confounds if we make this change.
Were steps taken to reduce demand characteristics?
- they didn't take any specific measure to reduce this?
How would you critique the design?
- attention check could help since they had many trials.
- their right/wrong criteria for incongurent sound cue trials are not fully justified: if someone hear an incongruent sound and report 'No' is that a wrong answer.
Can you think of any potential confounds to the study?
- "the presence of incongruent sound trials may have led participants to be more careful on these trials, inflating the RTs." (Edmiston & Lupyan, 2015)
Do you see any limitations to generalizability?
- all participants are US college undergrads.

## Results

### Data preparation

Data preparation following the analysis plan.

#### Pilot A Test (for Project Checkpoint 1)

We conducted pilot tests involving three participants. All data are collected using the [online experiment link](https://ucsd-psych201a.github.io/edmiston2015/). The data is available [here](https://github.com/ucsd-psych201a/edmiston2015/tree/main/data/pilot_a). 

#### Pilot B Test (for Project Checkpoint 3)

We conducted pilot tests involving five participants. All data are collected using Prolific. The data is available [here](https://github.com/ucsd-psych201a/edmiston2015/tree/main/data/pilot_b). The average time participant took to complete the study was 25 minutes.

#### Data cleaning and Data Exclusion Rules

Before testing the main hypothesis of the original study, we will first examine the average performance (accuracy) to determine if it is comparable to the original study. This will involve calculating each participant's accuracy and using a t-test to examine whether performance differs qualitatively between the original and new participants.

Trials with response times (RTs) that are too short or too long will be filtered out. The original study applied thresholds of 250 ms and 1500 ms for the lower and upper bounds, respectively. Before applying filtering, we will check if the new data shows a similar RT distribution.

Only RTs for correct responses on matching trials will be used for further analysis, following the original study’s approach.

The following code prepare the data for fitting a linear mixed model for response time under different conditions: (1) label cue matches the image, (2) auditory cue matches the image but is incongruent, and (3) auditory cue matches the image and is congruent. 
	
```{r include=T}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(readr)
library(lme4)

#### Import data
#replace this with your own path (for now)
folder_path <- "../data/pilot_b"
csv_files <- list.files(folder_path, pattern = "*.csv", full.names = TRUE)
df_list <- lapply(csv_files, read_csv, show_col_types = FALSE)
combined_df <- bind_rows(df_list)

#exclude practice trials 
combined_df <- combined_df %>%
  filter(exp_part == "actual")

#### Data exclusion / filtering
#exclude 'No' trials 
combined_df <- combined_df %>%
  mutate(correct_response = as.character(correct_response)) %>%
  mutate(response = as.character(response)) %>%
  mutate(correct = correct_response == response)

```

We check the accuracy of each participant:
```{r}
accuracy_table <- combined_df %>%
  group_by(ID) %>%
  summarise(accuracy = mean(correct, na.rm = TRUE))
accuracy_table
```

We set the threshold for data inclusion at 90% ^[We haven't discussed about it in details so this threshold may be a tentative one] accuracy. Four out of five participants in pilot B have achieved an acceptable performance (above 90%). The following is the code to further filter the data based on accuracy (for future use).

```{r}
accuracy_thresh <- 0.9
# select people who passed QA
id_pass_qa <- accuracy_table %>%
  filter(accuracy > accuracy_thresh) %>%
  pull(ID)
  
# filter combined_df
combined_df <- combined_df %>%
  filter(ID %in% id_pass_qa) %>%
  select(-correct) # drop the correctness column

# display
tail(combined_df)
```
We keep only trials whose correct response is 'Yes' and the time fall within 250-1500 ms:

```{r}
#check whether response is correct
combined_df <- combined_df %>%
  filter(response == "f") %>%
  filter(correct_response == "f")

#rename "sound_subtype" to "cue"
combined_df <- combined_df %>%
  rename(cue = sound_subtype)

#create congruency column 
combined_df <- combined_df %>%
  mutate(congruency = case_when(
    cue == "label" ~ "label",
    img_subtype %in% c("song", "york", "bongo", "acoustic", "harley", "rotary") & sound_version == "A" ~ "incongruent",
    TRUE ~ "congruent"
  ))


#filter reaction time 
combined_df <- combined_df %>%
  filter(rt >250, rt <1500)

#### Prepare data for analysis - create columns etc.
combined_df <- combined_df %>%
  select(rt, ID, sound_category,cue, congruency)

#show the preprocesssed data
tail(combined_df)
```

### Confirmatory analysis

#### Key Analysis

This analysis is to test whether (1) verbal labels elicit faster responses than sound labels, and (2) congruent sounds elicit faster response than incongruent sounds, with statistical significance, as suggested by the result of Experiment 1A from the original study.

As proposed in our analysis plan, we follow Edminston & Lupyan (2015) fitting reaction time for correct responses on matching trials under three conditions of different cues (verbal label cue, congruent sound cue, incongruent sound cue) using a linear mixed regression model. The model includes random intercepts and random slopes for within-subject factors and random intercepts for repeated items (unique trial types). The main effect (or “contrast of interest”) of the model will be the “condition” variable, which denotes whether a particular trial presented a label, congruent sound, or incongruent sound. 

In the following section, we will report the parameter estimates and confidence intervals for each “contrast of interest”. The parameter estimates for each condition should indicate how much each condition influences reaction time. Furthermore, to quantify our confidence in how much the factors of interest truly accounts for the trends we see, we conduct chi-square tests by comparing whether “nested models–models with and without the factor of interest–on improvement in log-likelihood”. We will the resulting p-value as the statistical significance of the effect.

We expect that the output of our model will be similar to the results presented in Edminston & Lupyan (2015).

The following code fit the mixed-effect linear regression model and reports the associated statistics.

```{r}
library(lmerTest)
library(emmeans)
library(lme4)

# compare between reference (congruent) and the other two values (incongruent and label)
model_full <- lmer(rt ~ congruency + (1 + congruency|ID) + (1|sound_category), data = combined_df)

summary(model_full)

#get 95% CI
confint.merMod(model_full,method="Wald")

#post-hoc test: examine relationship between incongruent and label
fit_result <- model_full %>% 
  emmeans(pairwise ~ congruency,
          #adjusts p values so that it is more difficult to get a significance (correction method)
          adjust = "bonferroni") %>% 
  pluck("contrasts")

fit_result
```
```{r, echo=FALSE}
library(dplyr)

# to read the fit result
get_contrast_values <- function(model_result, cond_a, cond_b) {
  # convert s4 to dataframe
  model_result <- summary(model_result)
  # Filter the model result for the specified contrast
  contrast_name <- paste0(cond_a, " - ", cond_b)
  result_row <- model_result[model_result$contrast == contrast_name, ]
  if (nrow(result_row) == 0) {
    # flip it
    contrast_name <- paste0(cond_b, " - ", cond_a)
    result_row <- model_result[model_result$contrast == contrast_name, ]
  }
  
  # Extract and return the estimate and p.value as a named list
  list(
    contrast_name = contrast_name,
    estimate = result_row$estimate,
    p_value = result_row$p.value
  )
}

congruent_incongruent_result <- get_contrast_values(
  fit_result, 'congruent', 'incongruent')
congruent_label_result <- get_contrast_values(
  fit_result, 'congruent', 'label')
incongruent_label_result <- get_contrast_values(
  fit_result, 'incongruent', 'label')
```

The result suggests that: 

- Comparing "congruent" and incongruent", the estimate of effect is `r paste0("\u03B2 = ", sprintf("%.2f", congruent_incongruent_result$estimate))` (`r congruent_incongruent_result$contrast_name`), with p-value `r sprintf("p = %.4f", congruent_incongruent_result$p_value)`;

- Comparing "congruent" and label", the estimate of effect is `r paste0("\u03B2 = ", sprintf("%.2f", congruent_label_result$estimate))` (`r congruent_label_result$contrast_name`), with p-value `r sprintf("p = %.4f", congruent_label_result$p_value)`;

- Comparing "incongruent" and label", the estimate of effect is `r paste0("\u03B2 = ", sprintf("%.2f", incongruent_label_result$estimate))` (`r incongruent_label_result$contrast_name`), with p-value `r sprintf("p = %.4f", incongruent_label_result$p_value)`;

To further test whether congruency has a meaningful impact on rt, we apply the chi-square test and the result is as follows:
```{r}
#chi square test 
model_reduced <- lmer(rt ~ (1 + congruency|ID) + (1|sound_category), data = combined_df)

anova(model_full, model_reduced)
```

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
