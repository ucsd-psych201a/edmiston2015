---
title: "Replication of Study What makes words special? Words as unmotivated cues (2015, Cognition)"
author: "Jenna Brooks(j8brooks@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Resources 
- [GitHub Repository](https://github.com/ucsd-psych201a/edmiston2015)
- [Pre-Registration](https://github.com/ucsd-psych201a/edmiston2015/tree/main/prereg) 
- [Paradigm](https://ucsd-psych201a.github.io/edmiston2015/)


## Introduction

This study aimed to explore why verbal labels, such as the words "dog" or "guitar," activate conceptual knowledge more effectively than environmental sounds associated with these objects, such as the bark of a dog or the strum of a guitar. Previous studies show that environmental sounds, like a dog bark, activate specific instances of a concept, while words, like "dog," cue broader, more abstract mental representations (Lupyan & Thompson-Schill, 2012). This study investigates this "label advantage" and tests whether verbal labels promote faster, more abstract mental processing compared to environmental sounds in an image recognition task.

This study finds that verbal labels (or words) are more effective than sounds in activating abstract category concepts because labels act as "unmotivated cues," broadly representing a category without specific reference to particular instances. In this experiment, participants will be presented with either a word or  environmental sound for the following categories: bird, dog, drum, guitar, motorcycle, and phone. Participants are presented with an auditory cue (either a word or a sound), followed by a picture displayed 1 second later, and are tested on how quickly and accurately they can determine whether the picture matches the auditory cue, with reaction times serving as the primary measure. This research seeks to deepen our understanding of how verbal labels enhance abstract categorization compared to environmental sounds by examining their influence on reaction time speed in an image recognition task.

## Methods

### Power Analysis 
Based on guidance from instructional staff, the sample size was determined with an a priori power analysis with the package simr, and is adequate to achieve at least 80% power for detecting the effect reported in the original study at a significance criterion of alpha = .05 (any random effects not specified in the original paper were taken from a small pilot study).

### Planned Sample
We plan to have a sample size of n = 50. For pre-screening, participants must speak English fluently and not have any hearing difficulties to ensure comprehension in the task. Participants are recruited and compensated on the Prolific online platform. 

### Materials

The materials were followed precisely as follows. All materials were provided by the original authors.  

"The auditory cues comprised basic-level category labels and environmental sounds for six categories: bird, dog, drum, guitar, motorcycle, and phone. For each category, we obtained two distinct environmental sound cues, e.g., <classical guitar strum>, <electric guitar strum>, and two separate images for each subordinate cate- gory, e.g., two electric guitars for <electric guitar strum>, two acoustic guitars for <electric guitar strum>. To control for cue variability, we also used two versions of each spoken category label: one pronounced by a female speaker, one by a male speaker. All auditory cues were equated in duration (600 ms.) and normalized in volume. The images were color photographs (four images per category). The materials, obtained from online repositories, are available for download at http://sapir.psych.wisc.edu/stimuli/ MotivatedCuesExp1A-1B.zip"(Edmiston & Lupyan, 2015).  

The link to our online experiment can be found [here](https://ucsd-psych201a.github.io/edmiston2015/)


### Procedure	

This procedure will be followed as closely as possible: 

"On each trial participants heard a cue and saw a picture. We instructed participants to decide as quickly and accurately as possible if the picture they saw came from the same basic-level cate- gory as the word or sound they heard Participants were tested in individual rooms sitting approximately 2400 from a monitor such that images subtended 10  10°. Trials began with a 250 ms. fixation cross followed immediately by the auditory cue, delivered via headphones. The target image appeared centrally 1 s after the off- set of the auditory cue and remained visible until a response was made. Each participant completed 6 practice and 384 test trials. If the picture matched the auditory cue (50% of trials) participants were instructed to respond ‘Yes’ on a gaming controller (e.g., <cell- phone ring> or ‘‘phone’’ followed by a picture of any phone). Otherwise, they were to press ‘No’ (e.g., <cellphone ring> or ‘‘phone’’ followed by a dog). All factors (cue type, congruence) var- ied randomly within subjects. Auditory feedback (buzz or bleep) was given after each trial"(Edmiston & Lupyan, 2015). 

We aim to follow the original procedure of Experiment 1A as precisely as possible. However, instead of running the trials in-person, the experiment will be conducted online using jsPsych. For this reason, the task will be slightly different such that participants will respond to trials using their keyboard keys instead of a gaming controller. We will also encourage participants to wear headphones, be in a quiet area for the auditory cues during the experiment, and provide an initial audio check to ensure that participants have access to all stimuli presented throughout the experiment. In addition, we did not have access to the exact instructions and prompts that the original researchers used, so these may differ slightly.  

### Analysis Plan

**Clarify key analysis of interest here**  
From the original study:

"All participants performed very accurately on all items (M = 97%). Response times (RTs) shorter than 250 ms. or longer than 1500 ms. were removed (292 trials removed, 1.77% of total).

We fit RTs for correct responses on matching trials (‘Yes’ responses) with linear mixed regression using maximum likelihood estimation (Bates, Maechler, Bolker, & Walker, 2013), including random intercepts and random slopes for within-subject factors and random intercepts for repeated items (unique trial types) following the recommendations of Barr, Levy, Scheepers, and Tily (2013). Reported below are the parameter estimates (b) and confidence intervals for each contrast of interest. Significance tests were calculated using chi-square tests that compared nested models—models with and without the factor of interest—on improvement in log-likelihood."

For our replication, we will also remove trials where response times are shorter than 250 ms or longer than 1500ms from the analysis. We will then run a similar **linear mixed regression model**, as was performed the original study. We will also use **chi-square tests** to assess significance of the results. We anticipate that a successful replication of the original study will yield similar effects. More specifically, we should find that **verbal labels elicit the lowest overall reaction times, and that congruent sounds elicit lower reaction times than incongruent sounds.**

### Differences from Original Study

This replication will differ from the original study in both setting and procedure. Instead of being conducted in a lab, the experiments will take place online using Prolific. Variations in participants' device performance and internet speed may influence reaction times. Additionally, participants will respond to trials using a keyboard rather than the gaming controller used previously. Despite these changes, we expect these differences will have minimal impact on the results or the ability to replicate the effect reported in the original study.

## Pilot A

For our Pilot A (with non-naive participants), we aimed to replicate the experimental paradigm described in the study. We collected an initial sample of 3 participants from friends. 

This data was imported and uploaded to the data folder in the project repository. 
[Pilot A data](https://github.com/ucsd-psych201a/edmiston2015/tree/main/data)

## Pilot B
For our Pilot B (with naive participants), we collected data from five participants on Prolific. The following "Results" section (Data Preparation and Confirmatory Analysis) is run on Pilot B data for the time-being. As predicted, the participants took an average of about 25 minutes to complete the experiment.

[Pilot B data](https://github.com/ucsd-psych201a/edmiston2015/tree/main/data/pilot_b)

## Pre-Registration
Our Pre-Registration can be found [here](https://github.com/ucsd-psych201a/edmiston2015/tree/main/prereg) 

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.
  
## Results

### Data preparation
We are replicating Experiment 1A from the paper. To clean the data we will remove incorrect responses and practice trials.  As for data exclusion rules, we will follow the parameters set by the original study in which "Response times (RTs) shorter than 250 ms. or longer than 1500 ms. [will be] removed". This accounts for outliers that may skew results. This data preparation also labels the conditions for later analysis (i.e. `congruent`, `incogruent`).
	
```{r}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(readr)
library(lme4)

#### Import data
#replace this with your own path (for now)
folder_path <- "../data/complete"
csv_files <- list.files(folder_path, pattern = "*.csv", full.names = TRUE)
df_list <- lapply(csv_files, read_csv)
combined_df <- bind_rows(df_list)

#### Data exclusion / filtering
#exclude 'No' trials 
combined_df <- combined_df %>%
  mutate(response = as.character(response)) %>%
  filter(response == "f")
#exclude practice trials 
combined_df <- combined_df %>%
  filter(exp_part == "actual")

#rename "sound_subtype" to "cue"
combined_df <- combined_df %>%
  rename(cue = sound_subtype)

#create congruency column 
combined_df <- combined_df %>%
  mutate(congruency = case_when(
    cue == "label" ~ "label",
    img_subtype %in% c("song", "york", "bongo", "acoustic", "harley", "rotary") & sound_version == "A" ~ "incongruent",
    TRUE ~ "congruent"
  ))


#filter reaction time 
combined_df <- combined_df %>%
  filter(rt >250, rt <1500)

#### Prepare data for analysis - create columns etc.
combined_df <- combined_df %>%
  select(rt, ID, sound_category,cue, congruency)
```
## Design Overview

Within-subjects design with 2 factors: Auditory Cue (Environmental Sound vs. Label) and Match to Basic Category (Match vs. No Match). Congruency is further manipulated within the Matching Environmental Sound condition.

Reaction time was the only measure taken. 

It uses within-participants design where everyone does each condition. 

Measures were repeated for 384 experimental trials. 

They didn't take any measures to reduce demand characteristics. 

To improve the experiment attention checks could be added. Given that incorrect responses could be labeled incongruent, incorrect responses could have also been included in the data. 
This sample could not be representative of all populations because it only studied undergraduates and WEIRD populations. 

### Confirmatory analysis
This analysis aims to determine whether (1) verbal labels lead to faster response times compared to sound labels, and (2) congruent sounds produce faster responses than incongruent sounds, in line with the findings of Experiment 1A from the original study.

As outlined in our analysis plan, we adopt the approach used by Edmiston & Lupyan (2015), modeling reaction times for correct responses in matching trials across three cue conditions (verbal label, congruent sound, incongruent sound) using a linear mixed-effects regression model. The model incorporates random intercepts and slopes for within-subject factors, as well as random intercepts for repeated measures (unique trial types). The primary variable of interest is the “condition,” representing whether the trial used a label, a congruent sound, or an incongruent sound.

In the next section, we will present parameter estimates and confidence intervals for each contrast of interest. These estimates indicate the extent to which each condition affects reaction times. Additionally, to assess how strongly the factors of interest explain the observed patterns, we conduct chi-square tests to compare nested models—those with and without the factor of interest—based on improvements in log-likelihood. The resulting p-values will indicate the statistical significance of the effects.

We anticipate that the model outputs will align closely with the results reported by Edmiston & Lupyan (2015). More specifically, incongruent environmental sounds will elicit longer response times in comparison to congruent environmental sounds. Verbal labels will elicit the shortest response time. 

*Side-by-side graph with original graph is ideal here*

```{r}
library(lmerTest)
library(emmeans)
library(lme4)

#recreation of fig 2
library(ggplot2)

# Reorder the 'congruency' factor levels
combined_df$congruency <- factor(combined_df$congruency, levels = c("label", "congruent", "incongruent"))

ggplot(data = combined_df,
       mapping = aes(x = congruency,
                     y = rt,
                     color = congruency, 
                     fill = congruency)) +
  geom_bar(stat = "summary", fun = "mean", 
           position = position_dodge(width = 0.6), width = 0.4, 
           color = "black") +  # Dark bands around the bars (black border)
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "linerange", 
               position = position_dodge(width = 0.6), color = "black") +
  scale_fill_manual(values = c("darkred", "darkblue", "lightblue")) +
  scale_color_manual(values = c("darkred", "darkblue", "lightblue")) +
  scale_x_discrete(labels = c("label" = "Label", 
                              "congruent" = "Congruent", 
                              "incongruent" = "Incongruent")) +  # Renaming x-axis labels
  labs(
    x = "Cue Type",
    y = "Reaction Time (Ms)",
    color = "Cue Type",
    fill = "Cue Type"
  ) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(panel.background = element_blank(), 
        panel.grid = element_blank(), 
        axis.line = element_line(color = "black"))



#statistical analysis 

#find best optimizer
#allFit(model_full)

# set congruent to back reference level
combined_df$congruency <- relevel(combined_df$congruency, ref = "congruent")

#set up model and view results
model_full <- lmer(rt ~ congruency + (1 + congruency|ID) + (1|sound_category), data = combined_df, control = lmerControl(optimizer = "bobyqa"))

summary(model_full)

# #get 95% CI
confint.merMod(model_full,method="Wald")


# #post-hoc test: allows you to examine relationship between incongruent and label
#####. 95% CI for this??? ###3
model_full %>%
  emmeans(pairwise ~ congruency,
          #adjusts p values so that it is more difficult to get a significance (correction method)
          adjust = "bonferroni", 
          conf.int = TRUE) %>%
  pluck("contrasts")

# #chi squared test 
model_reduced <- lmer(rt ~ (1 + congruency|ID) + (1|sound_category), data = combined_df)
anova(model_full, model_reduced)
```


*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
