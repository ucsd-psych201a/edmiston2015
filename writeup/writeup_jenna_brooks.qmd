---
title: "Replication of Study What makes words special? Words as unmotivated cues (2015, Cognition)"
author: "Jenna Brooks(j8brooks@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

This study aimed to explore why verbal labels, such as the words "dog" or "guitar," activate conceptual knowledge more effectively than environmental sounds associated with these objects, such as the bark of a dog or the strum of a guitar. I chose this topic because it intersects with my interests in language learning and auditory perception. This study finds that verbal labels (or words) are more effective than sounds in activating abstract category concepts because labels act as "unmotivated cues," broadly representing a category without specific reference to particular instances. In contrast, sounds are "motivated cues" that link directly to specific sources or instances, limiting their effectiveness in promoting conceptual abstraction. This difference is highlighted by experiments showing that words activate category-level knowledge more selectively than environmental sounds.

In this experiment, participants will be presented with either a verbal representation or  environmental sound for the following categories: bird, dog, drum, guitar, motorcycle, and phone. Participants are presented with an auditory cue (either a word or sound) and a picture presented 1 second after the auditory input is made. Participants are tested on how quickly and accurately they can determine if the picture presented matches the auditory cue they received. They will use a yes or no button on the computer screen. Potential challenges of this study could be sound quality of the environmental sounds to ensure they are clearly recognizable. Additionally, finding a diverse group of participants for this study could be a challenge. 

## Methods

### Power Analysis 
The use of linear mixed-effects models (lmer) with a complex structure limits our ability to perform a straightforward power analysis. This constraint makes it difficult to estimate effect sizes accurately, as we would in a simpler within-participants t-test design. We defaulted to expanding the sample size to 2.5 times the initial estimate. This buffer helps accommodate any unanticipated variance and ensures our study has enough statistical power to detect meaningful effects.


### Planned Sample
We plan to have a sample size of n = 108. For pre-screening, participants must speak English fluently to ensure comprehension in the task. Participants are recruited on Prolific online platform. 

### Materials

The materials were followed precisely as follows. "The auditory cues comprised basic-level category labels and environmental sounds for six categories: bird, dog, drum, guitar, motorcycle, and phone. For each category, we obtained two distinct environmental sound cues, e.g., <classical guitar strum>, <electric guitar strum>, and two separate images for each subordinate cate- gory, e.g., two electric guitars for <electric guitar strum>, two acoustic guitars for <electric guitar strum>. To control for cue variability, we also used two versions of each spoken category label: one pronounced by a female speaker, one by a male speaker. All auditory cues were equated in duration (600 ms.) and normalized in volume. The images were color photographs (four images per category). The materials, obtained from online repositories, are available for download at http://sapir.psych.wisc.edu/stimuli/ MotivatedCuesExp1A-1B.zip"(Edmiston & Lupyan, 2015).  

The link to our online experiment can be found here: https://ucsd-psych201a.github.io/edmiston2015/ 

### Procedure	

The procedure was followed precisely as in the original study for Experiment 1A. "On each trial participants heard a cue and saw a picture. We instructed participants to decide as quickly and accurately as possible if the picture they saw came from the same basic-level cate- gory as the word or sound they heard Participants were tested in individual rooms sitting approximately 2400 from a monitor such that images subtended 10  10°. Trials began with a 250 ms. fixation cross followed immediately by the auditory cue, delivered via headphones. The target image appeared centrally 1 s after the off- set of the auditory cue and remained visible until a response was made. Each participant completed 6 practice and 384 test trials. If the picture matched the auditory cue (50% of trials) participants were instructed to respond ‘Yes’ on a gaming controller (e.g., <cell- phone ring> or ‘‘phone’’ followed by a picture of any phone). Otherwise, they were to press ‘No’ (e.g., <cellphone ring> or ‘‘phone’’ followed by a dog). All factors (cue type, congruence) var- ied randomly within subjects. Auditory feedback (buzz or bleep) was given after each trial"(Edmiston & Lupyan, 2015). 


### Analysis Plan

**Clarify key analysis of interest here**  
A linear mixed regression model will be used to analyze response times for correct responses including "random intercepts and random slopes for within-subject factors and random intercepts for repeated items (unique trial types)"(Edmiston & Lupyan, 2015).  
In addition, significance tests using chi-square tests will be used to compare nested models, as was done in the original study, to assess the impact of factors with and without interest on response times. 

### Differences from Original Study

The original study used a remote controller.  The replication study will use the computer for responses and implement key responses to simulate buttons on controller. This should allow for accurate and quick responses. Participants were instructed to complete the study using headphones in a quiet environment. However, since the study was conducted online, experimenters had no control over participants' surroundings, which could introduce differences from the original study.

## Pilot A

For our Pilot A (with non-naive participants), we aimed to replicate the experimental paradigm described in the study. We collected an intial sampe of 3 participants from friends. The GitHub page for our paradigm is linked below:
[Link to experimental paradigm](https://ucsd-psych201a.github.io/edmiston2015/)

This data was imported and uploaded to the data folder in the project repository, which can be
found on our GitHub repoisitory here:

https://github.com/ucsd-psych201a/edmiston2015/tree/main/data


##Pilot B
We collected data for 5 participants. The average duration of the study was 25 minutes, exactly what we had predicted. 

##Pre-Registration
Our Pre-Registration can be found here: 

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.
  
## Results

### Data preparation
We are replicating Experiment 1A from the paper. To clean the data we will remove incorrect responses or participants who failed attention checks.  As for data exclusion rules, we will follow the parameters set by the original study in which "Response times (RTs) shorter than 250 ms. or longer than 1500 ms. [will be] removed". This accounts for outliers that may skew results. There aren't any covariates mentioned in this experiment. 
	
```{r}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(readr)
library(lme4)

#### Import data
#replace this with your own path (for now)
folder_path <- "../data/pilot_b"
csv_files <- list.files(folder_path, pattern = "*.csv", full.names = TRUE)
df_list <- lapply(csv_files, read_csv)
combined_df <- bind_rows(df_list)

#### Data exclusion / filtering
#exclude 'No' trials 
combined_df <- combined_df %>%
  mutate(response = as.character(response)) %>%
  filter(response == "j")
#exclude practice trials 
combined_df <- combined_df %>%
  filter(exp_part == "actual")

#rename "sound_subtype" to "cue"
combined_df <- combined_df %>%
  rename(cue = sound_subtype)

#create congruency column 
combined_df <- combined_df %>%
  mutate(congruency = case_when(
    cue == "label" ~ "label",
    img_subtype %in% c("song", "york", "bongo", "acoustic", "harley", "rotary") & sound_version == "A" ~ "incongruent",
    TRUE ~ "congruent"
  ))


#filter reaction time 
combined_df <- combined_df %>%
  filter(rt >250, rt <1500)

#### Prepare data for analysis - create columns etc.
combined_df <- combined_df %>%
  select(rt, ID, sound_category,cue, congruency)
```
## Design Overview
How many factors were manipulated?
In this experiment one factor, the auditory cue, was manipulated. Within this factor there are conditions - congruent sound, incongruent sound, and label. 

How many measures were taken?
Reaction time was the only measure taken. 

Did it use a within-participants or between-participants design?
It uses within-participants design where everyone does each condition. 

Were measures repeated?
Yes measures were repeated for 384 experimental trials. 

What would have been the consequence of changing one of these decisions? (e.g. moving from within to between participants). Could it have been either way?
There would be more confounds if it was between participants because people have different reaction times to stimuli, therefore the measures wouldn't be as accurate. 

Were steps taken to reduce demand characteristics?
They didn't take any measures to reduce demand characteristics. 

How would you critique the design?
To improve the experiment attention checks could be added. Given that incorrect responses could be labeled incongruent, incorrect responses could have also been included in the data. 

Can you think of any potential confounds to the study?

Do you see any limitations to generalizability?
This sample could not be representative of all populations because it only studied undergraduates and WEIRD populations. 

### Confirmatory analysis
Building on the methodology of Edminston & Lupyan (2015), we will analyze our response time data using mixed-effects linear regression models. This data will be filtered to include only correct responses from matching trials ("Yes" responses). The model will feature random intercepts and random slopes for within-subject factors (e.g., "subject") and random intercepts for repeated items (e.g., the specific item presented: bird, guitar, dog, etc.). The primary variable of interest ("contrast of interest") is the "condition," which specifies whether a trial involved a label, a congruent sound, or an incongruent sound. For each contrast of interest, we will report parameter estimates and confidence intervals (p. 94). Additionally, we will conduct chi-square tests to compare nested models—those with and without the factor of interest—by evaluating the improvement in log-likelihood (p. 94).

This analysis directly addresses the paper's key hypothesis: that the condition affects response times. By examining parameter estimates for the conditions (label, congruent, and incongruent), we can assess how each condition uniquely influences response time. Importantly, the model enables us to compare these effects across conditions. We anticipate that our results will closely align with those reported by Edminston & Lupyan (2015).

Finally, by constructing a model that excludes the factor of interest and comparing its performance to the full model, we aim to confirm whether this factor genuinely explains the observed patterns in the data.

*Side-by-side graph with original graph is ideal here*

```{r}
library(lmerTest)
library(emmeans)
library(lme4)

#shows you a comparsion between reference (congruent) and the other two values (incongruent and label)
model_full <- lmer(rt ~ congruency + (1 + congruency|ID) + (1|sound_category), data = combined_df)

summary(model_full)

#get 95% CI
confint.merMod(model_full,method="Wald")

#post-hoc test: allows you to examine relationship between incongruent and label
model_full %>% 
  emmeans(pairwise ~ congruency,
          #adjusts p values so that it is more difficult to get a significance (correction method)
          adjust = "bonferroni") %>% 
  pluck("contrasts")



#model shows us that incongruent takes longer than congruent b = 28.7, that label is shorter than congruent (b = -15.52), and that incongruent takes longer than label b = 44.3 (find 95% CI) 
```


*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
